# -*- coding: utf-8 -*-
"""preprocessing_dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t6pLEJFcq9CGRPn2Up2dYCwYRqJRzVIQ
"""

import pandas as pd
import itertools
import re
import math
from urllib.parse import urlparse
import numpy as np
import time
import matplotlib.pyplot as plt
import seaborn as sns
from nltk.tokenize import RegexpTokenizer # regexp tokenizers use to split words from text
from nltk.stem.snowball import SnowballStemmer # stemmes words
from sklearn.feature_extraction.text import CountVectorizer # create sparse matrix of words using regexptokenizes
from PIL import Image # getting images in notebook
from wordcloud import WordCloud

from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator# creates words colud

import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv('/content/phishing_site_urls.csv')

df.head(20)

df.shape

df.describe()

df['Label'].value_counts()

df['Label'] = df['Label'].replace({'good': 'safe', 'bad': 'malicious'})

df.head()

df.tail()

df = df.dropna(subset=['URL', 'Label'])

df = df.drop_duplicates(subset=['URL'])

df['URL'] = df['URL'].astype(str).str.strip().str.lower()

# Remove whitespace, unwanted characters
df['URL'] = df['URL'].apply(lambda u: re.sub(r'\s+', '', u))

df.shape

df.isnull().sum() # there is no missing values

#create a dataframe of classes counts
label_counts = pd.DataFrame(df.Label.value_counts())

#visualizing target_col
sns.set_style('darkgrid')
sns.barplot(x=label_counts.index, y=label_counts['count'])

tokenizer = RegexpTokenizer(r'[A-Za-z]+')

df.URL[0]

tokenizer.tokenize(df.URL[0])

print('Getting words tokenized ...')
t0= time.perf_counter()
df['text_tokenized'] = df.URL.map(lambda t: tokenizer.tokenize(t)) # doing with all rows
t1 = time.perf_counter() - t0
print('Time taken',t1 ,'sec')

df.sample(5)

# give root words
stemmer = SnowballStemmer("english")

print('Getting words stemmed ...')
t0= time.perf_counter()
df['text_stemmed'] = df['text_tokenized'].map(lambda l: [stemmer.stem(word) for word in l])
t1= time.perf_counter() - t0
print('Time taken',t1 ,'sec')

df.sample(10)

print('Getting joiningwords ...')
t0= time.perf_counter()
df['text_sent'] = df['text_stemmed'].map(lambda l: ' '.join(l))
t1= time.perf_counter() - t0
print('Time taken',t1 ,'sec')

df.sample(5)

!pip install tldextract

import tldextract

def extract_features(df):
    suspicious_keywords = [
        'login', 'verify', 'update', 'secure', 'account', 'bank', 'free', 'bonus', 'gift', 'signin'
    ]

    df['url_length'] = df['URL'].apply(len)
    df['num_dots'] = df['URL'].apply(lambda x: x.count('.'))
    df['num_digits'] = df['URL'].apply(lambda x: sum(c.isdigit() for c in x))
    df['num_special_chars'] = df['URL'].apply(lambda x: len(re.findall(r'[-_@%&?=+]', x)))
    df['num_subdirs'] = df['URL'].apply(lambda x: x.count('/'))

    df['has_ip_address'] = df['URL'].apply(
        lambda x: 1 if re.search(r'(\d{1,3}\.){3}\d{1,3}', x) else 0
    )

    df['is_https'] = df['URL'].apply(lambda x: 1 if x.startswith('https') else 0)

    # ✅ Robust TLD extraction
    df['tld'] = df['URL'].apply(
        lambda x: tldextract.extract(str(x)).suffix if pd.notnull(x) else 'unknown'
    )

    df['tld'] = df['tld'].replace('', 'unknown').fillna('unknown')

    df['has_suspicious_words'] = df['URL'].apply(
        lambda x: 1 if any(word in x.lower() for word in suspicious_keywords) else 0
    )

    df['entropy'] = df['URL'].apply(shannon_entropy)

    return df

df = extract_features(df)

#sliceing classes
bad_sites = df[df.Label == 'malicious']
good_sites = df[df.Label == 'safe']

bad_sites.head(20)

good_sites.head()

def plot_wordcloud(text, mask=None, max_words=400, max_font_size=120, figure_size=(24.0,16.0),
                   title = None, title_size=40, image_color=False):
    stopwords = set(STOPWORDS)
    more_stopwords = {'com','http'}
    stopwords = stopwords.union(more_stopwords)

    wordcloud = WordCloud(background_color='white',
                    stopwords = stopwords,
                    max_words = max_words,
                    max_font_size = max_font_size,
                    random_state = 42,
                    mask = mask)
    wordcloud.generate(text)

    plt.figure(figsize=figure_size)
    if image_color:
        image_colors = ImageColorGenerator(mask);
        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation="bilinear");
        plt.title(title, fontdict={'size': title_size,
                                  'verticalalignment': 'bottom'})
    else:
        plt.imshow(wordcloud);
        plt.title(title, fontdict={'size': title_size, 'color': 'green',
                                  'verticalalignment': 'bottom'})
    plt.axis('off');
    plt.tight_layout()

data = good_sites.text_sent
data.reset_index(drop=True, inplace=True)

common_text = str(data)
common_mask = np.array(Image.open('star.png'))
plot_wordcloud(common_text, common_mask, max_words=400, max_font_size=120,
               title = 'Most common words use in good urls', title_size=15)

common_text = str(data)
common_mask = np.array(Image.open('comment.png'))
plot_wordcloud(common_text, common_mask, max_words=400, max_font_size=120,
               title = 'Most common words use in bad urls', title_size=15)

# Save the preprocessed and feature-engineered dataset to a CSV file
processed_file_path = '/content/processed_phishing_urls.csv'  # or any preferred path
df.to_csv(processed_file_path, index=False)

print(f"✅ Preprocessed dataset saved successfully at: {processed_file_path}")
print(f"Shape of saved dataset: {df.shape}")

