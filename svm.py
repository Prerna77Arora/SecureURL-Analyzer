# -*- coding: utf-8 -*-
"""Xgboost.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tvukciWzyLZO4aqlRTBrKx_4lP_XGnzM
"""

# 1Ô∏è‚É£ Import Libraries
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import LinearSVC

from sklearn.metrics import (
    accuracy_score, confusion_matrix, classification_report,
    precision_score, recall_score, f1_score
)
from scipy.sparse import hstack
import seaborn as sns
import matplotlib.pyplot as plt
import joblib  # for saving model
import torch

df = pd.read_csv('/content/processed_phishing_urls.csv')

# Check for GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

df.head(20)

df.Label.value_counts()

df.shape

df.dtypes

print("\n‚öñÔ∏è Class distribution:\n", df['Label'].value_counts())
sns.countplot(x='Label', data=df, palette='pastel')
plt.title('Class Distribution')
plt.xlabel('Label')
plt.ylabel('Count')
plt.show()

# 4Ô∏è‚É£ Prepare features
cv = CountVectorizer()
X_text = cv.fit_transform(df['text_sent'].fillna(''))
# One-hot encode TLD (Top-Level Domain)
tld_encoded = pd.get_dummies(df['tld'].fillna('unknown'), prefix='tld')

numeric_features = df[['url_length', 'num_dots', 'num_digits', 'num_special_chars',
                       'num_subdirs', 'has_ip_address', 'is_https',
                       'has_suspicious_words', 'entropy']]

# Combine text, TLD, and numeric features
X = hstack([X_text, tld_encoded.values, numeric_features.values])
y = df['Label']

# Ensure labels are numeric (if strings)
if y.dtype == 'object':
    y = y.map({'malicious': 1, 'safe': 0}).fillna(y).astype(int)

# 5Ô∏è‚É£ Train-test split (with stratification)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

model = LinearSVC(
    class_weight='balanced',   # handle class imbalance
    max_iter=5000,             # ensure convergence
    C=1.0                      # regularization parameter
)

model.fit(X_train, y_train)

#  Predictions
y_pred_train = model.predict(X_train)
y_pred_test = model.predict(X_test)

# 8Ô∏è‚É£ Evaluation Metrics
train_acc = accuracy_score(y_train, y_pred_train)
test_acc = accuracy_score(y_test, y_pred_test)
precision = precision_score(y_test, y_pred_test)
recall = recall_score(y_test, y_pred_test)
f1 = f1_score(y_test, y_pred_test)

print("\nüîπ Training Accuracy:", train_acc)
print("üîπ Testing Accuracy:", test_acc)
print("üîπ Precision:", precision)
print("üîπ Recall:", recall)
print("üîπ F1 Score:", f1)

print("\nClassification Report:\n", classification_report(y_test, y_pred_test))

# 9Ô∏è‚É£ Confusion Matrix
cm = confusion_matrix(y_test, y_pred_test)
plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Greens')
plt.title('Confusion Matrix - Random Forest')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# üîü Save Model and Vectorizer
joblib.dump(model, 'SVM.pkl')
joblib.dump(cv, 'countvectorizer.pkl')  # Save the vectorizer too